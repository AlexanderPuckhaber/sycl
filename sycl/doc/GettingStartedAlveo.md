Getting started with SYCL with a Xilinx FPGA U200 Alveo board and Ubuntu 20.04
==============================================================================

Disclaimer: nothing here is supported and this is all about a research
project.

We assume that you have the latest Ubuntu 20.04 version installed on
an `x86_64` machine. But it might work for some other version of Ubuntu or
Debian with some adaptations.


## Installing the Alveo U200 board

If you do not have a real board and want to use only software or
hardware emulation, just skip this section.

Install an Alveo U200 board in the machine with the right cooling.
The PCIe auxiliary power is not necessary for simple tests not using the full
power of the board.


### Use a modern BIOS

Update the BIOS of your machine to the latest version. The Alveo U200
board might not be detected by `lspci` at the PCIe level if you do not
have an up-to-date BIOS.

If you are running Linux on top of an EFI BIOS, you can probably use
the firmware capsule concept and try:
```bash
# Install the firmware manager and daemon
sudo apt install fwupdate fwupd
# Refresh the list of available firmware
sudo fwupdmgr refresh
# Do the firmware update if any
sudo fwupdmgr update
```

If you are not running an EFI BIOS, you can follow the manual BIOS
update recipe for your motherboard. Typically look for the
latest BIOS version, put it on a FAT32-formatted USB stick and go
into the BIOS setup at boot time to ask for the explicit
update. Often, there is no need to build a bootable USB stick.


## Installing the Xilinx runtime

```bash
# Get the latest Xilinx runtime. You might try the master branch instead...
# Use either ssh
git clone git@github.com:Xilinx/XRT.git
# or https according to your usual method
git clone https://github.com/Xilinx/XRT.git
cd XRT/build
# Install the required packages
sudo ../src/runtime_src/tools/scripts/xrtdeps.sh
# Compile the Xilinx runtime
./build.sh
# Install the runtime into /opt/xilinx/xrt and compile/install
# the Linux kernel drivers (adapt to the real name if different)
sudo apt install --reinstall ./Release/xrt_202020.2.8.0_20.04-amd64-xrt.deb
```

It will install the user-mode XRT runtime and at least compile and
install the Xilinx device driver modules for the current running kernel,
even if it fails for the other kernels installed on the machine. If
you do not plan to run on a real FPGA board but only use software or
hardware emulation instead, it does not matter if the kernel device
driver is not compiled since it will not be used.

Note that if for some reasons you want to use a debug version of XRT,
use this recipe instead:
```bash
cd Debug
# You need to make explicitly the Debug package because it is not made
# by default
make package
# Install the runtime into /opt/xilinx/xrt and compile/install
# the Linux kernel drivers (adapt to the real name if different)
sudo apt install --reinstall ./xrt_202020.2.8.0_20.04-amd64-xrt.deb
```

Check that the FPGA board is detected:
```bash
sudo /opt/xilinx/xrt/bin/xbutil flash scan -v
XBFLASH -- Xilinx Card Flash Utility
Card [0]
	Card BDF:		0000:04:00.0
	Card type:		u200
	Flash type:		SPI
	Shell running on FPGA:
		xilinx_u200_GOLDEN_2,[SC=1.8]
	Shell package installed in system:	(None)
	Card name		AU200P64G
	Card S/N: 		2130048BQ00H
	Config mode: 		7
	Fan presence:		P
	Max power level:	75W
	MAC address0:		00:0A:35:05:EF:0E
	MAC address1:		00:0A:35:05:EF:0F
	MAC address2:		FF:FF:FF:FF:FF:FF
	MAC address3:		FF:FF:FF:FF:FF:FF
```


## Install Vitis 2020.1

You need the framework to do the hardware synthesis, taking the SPIR
intermediate representation generated by the SYCL compiler (or HLS C++
or OpenCL or...) and generating some FPGA configuration bitstream.


Download from
https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/vitis.html
the "Xilinx Vitis 2020.1: All OS installer Single-File Download".

Create a `/opt/xilinx` with the right access mode so you can work in
it.

Materialize the installer files with
```bash
cd /opt/xilinx
tar zxvf .../Xilinx_Unified_2020.1_0602_1208.tar.gz
```

Since the graphics Java installer does not work on modern Linux
distributions like Ubuntu 20.04, use the batch-mode version:
```bash
/opt/xilinx/Xilinx_Unified_2020.1_0602_1208/xsetup --batch Install --location /opt/xilinx --agree 3rdPartyEULA,WebTalkTerms,XilinxEULA --edition "Vitis Unified Software Platform"
```
and select `Vitis`.

FIXME
Ask for a 30 day trial license for example. It puts some information
into your `~/.Xilinx`.

FIXME
Vitis comes with some old useless pieces of XRT to
generate/instrospect the bitstream containers and still wants to use
them... So we need to patch SDx to use the right XRT by executing:
```bash
sudo ln -s $XILINX_XRT /opt/xilinx/SDx/2019.1/xrt/xrt-2.1.0-ubuntu19.04
```


## Install the target platform for the FPGA board

To execute some kernels on the FPGA board you need a deployment target platform
which is an FPGA configuration that pre-defines an architecture on the
FPGA to execute some kernels in some reconfigurable area.

To develop some kernels for your FPGA board or to run some
simulations, you need a development target platform that will contain
some internal description specific to the FPGA and its deployment
target platform so that the tools can generate the right bitstream for
the kernels or the simulation details.

Pick the latest deployment and development target platforms from
https://www.xilinx.com/products/boards-and-kits/alveo/u200.html#gettingStarted
for your board and for you OS. It might be an older version. For
example it is possible to use a `2018.3` target platform with Vitis `2020.1`,
and a version for Ubuntu `18.04` on a more recent version of Ubuntu or
Debian.

Install the target platforms:
```bash
sudo apt install ./xilinx-u200-xdma-201830.2-2580015_18.04.deb
sudo apt install ./xilinx-u200-xdma-201830.2-dev-2580015_18.04.deb
```
from where they have been downloaded or adapt the paths to them.


### Flash the board

If you do not want to use a real board, skip this section.

If you want to use a real board, follow the recipe "Generating the
xbutil flash Command" from
https://www.xilinx.com/html_docs/accelerator_cards/alveo_doc/ftt1547585535561.html
about how to correctly generate the exact flashing command.

Typically you run:
```bash
sudo /opt/xilinx/xrt/bin/xbutil flash scan
XBFLASH -- Xilinx Card Flash Utility
Card [0]
	Card BDF:		0000:04:00.0
	Card type:		u200
	Flash type:		SPI
	Shell running on FPGA:
		xilinx_u200_GOLDEN_2,[SC=1.8]
	Shell package installed in system:	
		xilinx_u200_xdma_201830_1,[TS=0x000000005bece8e1],[SC=3.1]
```
to get the information about the installed target platform and you
translate this into a flashing command according to the parameters
above or just follow the information you got when installing
previously the deployment target platform:
```bash
WARNING: The xbutil sub-command flash has been deprecated. Please use the xbmgmt utility with flash sub-command for equivalent functionality.

         Status: shell needs updating
         Current shell: xilinx_u200_xdma_201830_2
         Shell to be flashed: xilinx_u200_xdma_201830_2
Are you sure you wish to proceed? [y/n]: y

Updating shell on card[0000:04:00.0]
Bitstream guard installed on flash @0x1002000
Persisted 451594 bytes of meta data to flash 0 @0x7f91bca
Extracting bitstream from MCS data:
............................................
Extracted 45859024 bytes from bitstream @0x1002000
Writing bitstream to flash 0:
............................................
Bitstream guard removed from flash
Successfully flashed Card[0000:04:00.0]

1 Card(s) flashed successfully.
Cold reboot machine to load the new image on card(s).
```

Unfortunately you need to "cold reboot" the machine to have the new
target platform loaded inside the FPGA, which means to really
power-off the machine so the new instantiated PCIe interface of the
card can actually be rediscovered by the host machine (everything is
configurable with an FPGA!).

Then after rebooting, you can check with a pre-compiled FPGA program
provided by the target platform that the board is working correctly
with:
FIXME
```bash
sudo /opt/xilinx/xrt/bin/xbutil validate
INFO: Found 1 cards

INFO: Validating card[0]: xilinx_u200_xdma_201830_1
INFO: Checking PCIE link status: PASSED
INFO: Starting verify kernel test: 
INFO: verify kernel test PASSED
INFO: Starting DMA test
Host -> PCIe -> FPGA write bandwidth = 11127.5 MB/s
Host <- PCIe <- FPGA read bandwidth = 12147.9 MB/s
INFO: DMA test PASSED
INFO: Starting DDR bandwidth test: ..........
Maximum throughput: 47665.777344 MB/s
INFO: DDR bandwidth test PASSED
INFO: Starting P2P test
P2P BAR is not enabled. Skipping validation
INFO: P2P test PASSED
INFO: Starting M2M test
bank0 -> bank1 M2M bandwidth: 12089.7 MB/s	
bank0 -> bank2 M2M bandwidth: 12064.7 MB/s	
bank0 -> bank3 M2M bandwidth: 12084 MB/s	
bank1 -> bank2 M2M bandwidth: 12064.1 MB/s	
bank1 -> bank3 M2M bandwidth: 12066.9 MB/s	
bank2 -> bank3 M2M bandwidth: 12125.8 MB/s	
INFO: M2M test PASSED
INFO: Card[0] validated successfully.

INFO: All cards validated successfully.
```


## Compile the SYCL compiler

For details about LLVM's CMake configuration see https://llvm.org/docs/CMake.html
but it is possible to use the simpler Python scripts to build the SYCL
environment:
```
# Pick some place where SYCL has to be compiled:
SYCL_HOME=~/sycl_workspace
mkdir $SYCL_HOME
cd $SYCL_HOME
git clone --branch sycl/unified/master git@github.com:triSYCL/sycl.git llvm
python $SYCL_HOME/llvm/buildbot/configure.py
python $SYCL_HOME/llvm/buildbot/compile.py
```

## Compiling and running a SYCL application

The typical environment is setup with something like
FIXME
```bash
# The place where SYCL has been compiled:
SYCL_HOME=~/sycl_workspace
XILINX_VERSION=2020.1
# The target platform for the FPGA board model
export XILINX_PLATFORM=xilinx_u200_xdma_201830_2
# Where all the Xilinx tools are
XILINX_ROOT=/opt/xilinx
export XILINX_XRT=$XILINX_ROOT/xrt
export XILINX_VITIS=$XILINX_ROOT/Vitis/$XILINX_VERSION
# For some tests? FIXME
export XILINX_SDX=$XILINX_VITIS
export XILINX_VIVADO=$XILINX_ROOT/Vivado/$XILINX_VERSION
PATH=$PATH:$XILINX_XRT/bin:$XILINX_SDX/bin:$XILINX_VIVADO/bin
# Update to the real place the SYCL compiler working tree is:
SYCL_BIN_DIR=$SYCL_HOME/llvm/bin
PATH=$PATH:$SYCL_BIN_DIR:$XILINX_XRT/bin:/opt/xilinx/Vitis/2020.1/bin:/opt/xilinx/Vivado/2020.1/bin
export LD_LIBRARY_PATH=$XILINX_XRT/lib:$SYCL_HOME/llvm/build/lib:$LD_LIBRARY_PATH
# Setup LIBRARY_PATH used in hw and hw_emu mode
# Ask ldconfig about the list of system library directories
export LIBRARY_PATH=$(ldconfig --verbose 2>/dev/null | grep ':$' | tr -d '\n')
# Setup device to be used in simulation mode.
# Instead of running emconfigutil all over the place with
emconfig.json everywhere, put once at a common place:
export EMCONFIG_PATH=~/.Xilinx
emconfigutil --platform $XILINX_PLATFORM --od $EMCONFIG_PATH --save-temps
```

You can compile an application either for real FPGA execution,
software emulation (the SYCL device code is executed by the XRT
runtime on CPU) or hardware emulation (the SYCL device code is
synthesized into RTL Verilog and run by an RTL simulator such as
`xsim`).

Note that the software and hardware emulation might not work for some
system incompatibility reasons because Vitis comes with a lot of
system-specific assumptions with a lot of old compilers and libraries
instead of just using the ones from the system and the mix-and-match
might be wrong on your current system... But the hardware execution
just requires the open-source XRT that should have been compiled just
using what is available on the system.

The `XCL_EMULATION_MODE` environment variable selects the compilation &
execution mode and is used by the SYCL compiler and XRT runtime.

So to run an example, for example start with
- with software emulation:
  compile and run a single file
  ```bash
  cd $SYCL_HOME/llvm/sycl/test/xocc_tests/simple_tests
  # Instruct the compiler and runtime to use FPGA software emulation
  export XCL_EMULATION_MODE=sw_emu
  # Compile the SYCL program down to a host fat binary including device code for CPU
  $SYCL_BIN_DIR/clang++ -std=c++20 -fsycl -fsycl-targets=fpga64-xilinx-unknown-sycldevice \
    parallel_for_ND_range.cpp -o parallel_for_ND_range
  # Run the software emulation
  ./parallel_for_ND_range
  ```
  run the test suite
  this takes usually 4-6 minutes with a good CPU
  ```bash
  export XCL_EMULATION_MODE=sw_emu
  make -j`nrpoc` check-sycl-xocc-jmax
  ```
- with hardware emulation:
  ```bash
  # Instruct the compiler and runtime to use FPGA hardware emulation
  export XCL_EMULATION_MODE=hw_emu
  # Compile the SYCL program down to a host fat binary including the RTL for simulation
  $SYCL_BIN_DIR/clang++ -std=c++20 -fsycl -fsycl-targets=fpga64-xilinx-unknown-sycldevice \
    parallel_for_ND_range.cpp -o parallel_for_ND_range
  # Run the hardware emulation
  ./parallel_for_ND_range
  ```
  run the test suite
  this takes usually 15-30 minutes with a good CPU
  ```bash
  export XCL_EMULATION_MODE=hw_emu
  make -j`nrpoc` check-sycl-xocc-j4
  ```
- with real hardware execution on FPGA:
  ```bash
  # Instruct the compiler and runtime to use real FPGA hardware execution
  export XCL_EMULATION_MODE=hw
  # Compile the SYCL program down to a host fat binary including the FPGA bitstream
  $SYCL_BIN_DIR/clang++ -std=c++20 -fsycl -fsycl-targets=fpga64-xilinx-unknown-sycldevice \
    parallel_for_ND_range.cpp -o parallel_for_ND_range
  # Unset the variable at execution time to have real execution
  unset XCL_EMULATION_MODE
  # Run on the real FPGA board
  ./parallel_for_ND_range
  ```
  run the test suite
  this takes usually 8+ hours
  ```bash
  export XCL_EMULATION_MODE=hw
  make -j`nrpoc` check-sycl-xocc-j4
  ```
Note that the compilation line does not change, just the environment variable.

check-sycl-xocc-jmax will run the tests on as many cores as is available on the system.
but for hw and hw_emu this usually means the system will run out of RAM even with 64G
so check-sycl-xocc-j4 should be used, it will run only 4 tests in parallel.

### Running a bigger example on real FPGA

To run a SYCL translation of
https://github.com/Xilinx/SDAccel_Examples/tree/master/vision/edge_detection
```bash
cd $SYCL_HOME/llvm/sycl/test/xocc_tests/disabled/edge_detection
export XCL_EMULATION_MODE=sw_emu
$SYCL_BIN_DIR/clang++ -std=c++20 -fsycl \
    -fsycl-targets=fpga64-xilinx-unknown-sycldevice edge_detection.cpp \
    -o edge_detection `pkg-config --libs --cflags opencv4`
# Execute on one of the images
./edge_detection data/input/eiffel.bmp
```
and then look at the `input.bmp` and `output.bmp` images.

There is another application using a webcam instead, if you have one
on your machine.


## Cleaning up some buffer allocation

The XRT memory model is richer (and more complex...) than the
OpenCL memory model: buffers can be allocated on some DDR or HBM
memory banks, buffers can be shared between different processes on the
host, etc.

This means that the buffer lifetime is actually handled by the `xocl`
kernel driver across the Linux system image to manage this memory
sharing across different processes, if required. The OpenCL buffer
creation and destruction APIs handle this and fortunately this is
hidden by the higher-level SYCL framework.

But if a SYCL program crashes before deallocating the OpenCL buffer
and the user tries to allocate some other buffers at the same place on
the FPGA board with another program, then the runtime refuses to load
the program, with some error like:
```
[XRT] ERROR: Failed to load xclbin.
OpenCL API failed. /var/tmp/rkeryell/SYCL/sycl/sycl/source/detail/program_manager/program_manager.cpp:78: OpenCL API returns: -44 (CL_INVALID_PROGRAM)
```
and with some kernel message that can be displayed by executing `dmesg`
like:
```
[256564.482271] [drm] Finding MEM_TOPOLOGY section header
[256564.482273] [drm] Section MEM_TOPOLOGY details:
[256564.482274] [drm]   offset = 0x29e5908
[256564.482275] [drm]   size = 0x120
[256564.482282] xocl 0000:04:00.1: xocl_check_topology: The ddr 0 has pre-existing buffer allocations, please exit and re-run.
[256564.482287] xocl 0000:04:00.1: xocl_read_axlf_helper: err: -1
```

Then you need to explicitly deallocate the buffer because the device
driver still has the hope a program wants to use the data of the
allocated buffer in the future...

This can be done by removing the kernel driver and reloading it by
executing:
```bash
sudo rmmod xocl
sudo modprobe xocl
```
